{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base directory\n",
    "base_dir = \"/Users/aarohiverma/Documents/Chameleon/test/\"\n",
    "\n",
    "# Initialize lists to hold the paths and labels\n",
    "data = []\n",
    "\n",
    "# Loop over the two subfolders\n",
    "for label_str, label_int in [(\"0_real\", 0), (\"1_fake\", 1)]:\n",
    "    folder_path = os.path.join(base_dir, label_str)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):  # add more if needed\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            data.append((full_path, label_int))\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=['path', 'label'])\n",
    "\n",
    "# Optional: shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n",
    "df.to_csv(\"chameleon.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T19:58:06.305062Z",
     "iopub.status.busy": "2025-04-22T19:58:06.304757Z",
     "iopub.status.idle": "2025-04-22T20:17:48.096022Z",
     "shell.execute_reply": "2025-04-22T20:17:48.094918Z",
     "shell.execute_reply.started": "2025-04-22T19:58:06.305039Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890M/890M [00:34<00:00, 27.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed for 80_448 set!\n",
      "Feature extraction completed for 20_448 set!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# Modify preprocessing (larger image size, optional)\n",
    "custom_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    preprocess.transforms[-3],\n",
    "    preprocess.transforms[-2],\n",
    "    preprocess.transforms[-1],\n",
    "])\n",
    "\n",
    "# Load and shuffle dataset\n",
    "df = pd.read_csv(\"/kaggle/input/detect-ai-vs-human-generated-images/train.csv\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "# Manual 80-20 split\n",
    "split_idx = int(0.8 * len(df))\n",
    "df_80 = df.iloc[:split_idx].reset_index(drop=True)\n",
    "df_20 = df.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "# Dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, base_path, transform):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f\"{self.base_path}/{self.df.iloc[idx]['file_name']}\"\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        return image, label\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(df_split, name):\n",
    "    dataset = ImageDataset(df_split, \"/kaggle/input/ai-vs-human-generated-dataset\", custom_preprocess)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, lbls in dataloader:\n",
    "            images = images.to(device)\n",
    "            batch_features = model.encode_image(images).cpu().numpy()\n",
    "            features.append(batch_features)\n",
    "            labels.append(lbls.numpy())\n",
    "\n",
    "    X = np.vstack(features)\n",
    "    y = np.concatenate(labels)\n",
    "\n",
    "    np.save(f\"clip_features_{name}.npy\", X)\n",
    "    np.save(f\"clip_labels_{name}.npy\", y)\n",
    "    print(f\"Feature extraction completed for {name} set!\")\n",
    "\n",
    "# Run for both splits\n",
    "extract_features(df_80, \"80_448\")\n",
    "extract_features(df_20, \"20_448\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T15:49:40.781508Z",
     "iopub.status.busy": "2025-04-23T15:49:40.781177Z",
     "iopub.status.idle": "2025-04-23T15:51:34.920159Z",
     "shell.execute_reply": "2025-04-23T15:51:34.919323Z",
     "shell.execute_reply.started": "2025-04-23T15:49:40.781477Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.3836, Accuracy: 81.45%\n",
      "Epoch 2/40, Loss: 0.1396, Accuracy: 94.89%\n",
      "Epoch 3/40, Loss: 0.1003, Accuracy: 96.26%\n",
      "Epoch 4/40, Loss: 0.0837, Accuracy: 96.95%\n",
      "Epoch 5/40, Loss: 0.0730, Accuracy: 97.31%\n",
      "Epoch 6/40, Loss: 0.0657, Accuracy: 97.59%\n",
      "Epoch 7/40, Loss: 0.0608, Accuracy: 97.81%\n",
      "Epoch 8/40, Loss: 0.0562, Accuracy: 97.94%\n",
      "Epoch 9/40, Loss: 0.0528, Accuracy: 98.08%\n",
      "Epoch 10/40, Loss: 0.0472, Accuracy: 98.31%\n",
      "Epoch 11/40, Loss: 0.0432, Accuracy: 98.42%\n",
      "Epoch 12/40, Loss: 0.0413, Accuracy: 98.51%\n",
      "Epoch 13/40, Loss: 0.0377, Accuracy: 98.62%\n",
      "Epoch 14/40, Loss: 0.0363, Accuracy: 98.67%\n",
      "Epoch 15/40, Loss: 0.0322, Accuracy: 98.85%\n",
      "Epoch 16/40, Loss: 0.0317, Accuracy: 98.86%\n",
      "Epoch 17/40, Loss: 0.0315, Accuracy: 98.86%\n",
      "Epoch 18/40, Loss: 0.0297, Accuracy: 98.95%\n",
      "Epoch 19/40, Loss: 0.0278, Accuracy: 99.00%\n",
      "Epoch 20/40, Loss: 0.0293, Accuracy: 98.91%\n",
      "Epoch 21/40, Loss: 0.0268, Accuracy: 99.03%\n",
      "Epoch 22/40, Loss: 0.0284, Accuracy: 98.99%\n",
      "Epoch 23/40, Loss: 0.0254, Accuracy: 99.02%\n",
      "Epoch 24/40, Loss: 0.0228, Accuracy: 99.18%\n",
      "Epoch 25/40, Loss: 0.0236, Accuracy: 99.16%\n",
      "Epoch 26/40, Loss: 0.0233, Accuracy: 99.15%\n",
      "Epoch 27/40, Loss: 0.0229, Accuracy: 99.15%\n",
      "Epoch 28/40, Loss: 0.0195, Accuracy: 99.26%\n",
      "Epoch 29/40, Loss: 0.0204, Accuracy: 99.26%\n",
      "Epoch 30/40, Loss: 0.0221, Accuracy: 99.22%\n",
      "Epoch 31/40, Loss: 0.0210, Accuracy: 99.25%\n",
      "Epoch 32/40, Loss: 0.0179, Accuracy: 99.36%\n",
      "Epoch 33/40, Loss: 0.0180, Accuracy: 99.37%\n",
      "Epoch 34/40, Loss: 0.0178, Accuracy: 99.35%\n",
      "Epoch 35/40, Loss: 0.0174, Accuracy: 99.38%\n",
      "Epoch 36/40, Loss: 0.0168, Accuracy: 99.42%\n",
      "Epoch 37/40, Loss: 0.0173, Accuracy: 99.37%\n",
      "Epoch 38/40, Loss: 0.0171, Accuracy: 99.42%\n",
      "Epoch 39/40, Loss: 0.0177, Accuracy: 99.33%\n",
      "Epoch 40/40, Loss: 0.0167, Accuracy: 99.39%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Load CLIP features and labels\n",
    "features = np.load(\"/kaggle/input/clip-embeddings-test/clip_features_80_448.npy\")\n",
    "labels = np.load(\"/kaggle/input/clip-embeddings-test/clip_labels_80_448.npy\")\n",
    "\n",
    "# Normalize Features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "indices = np.random.permutation(len(features))\n",
    "features = features[indices]\n",
    "labels = labels[indices] \n",
    "\n",
    "# Convert to Torch Tensors\n",
    "features = torch.tensor(features, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)  # Ensure it's (N,1)\n",
    "\n",
    "# Split into Train & Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create PyTorch DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define MLP Model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "\n",
    "            nn.Linear(256, 256),  # ðŸ”¹ Keep 256 Instead of 128\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Initialize Model\n",
    "input_dim = features.shape[1]  # Either 768 or 1024\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLPClassifier(input_dim).to(device)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for Classification\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)  # ðŸ”¹ AdamW with weight decay\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        preds = (outputs > 0.5).float()\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few shot training base model on Chameleon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load CLIP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# Custom preprocessing (preserving CLIP normalization)\n",
    "custom_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    preprocess.transforms[-3],  # ToTensor\n",
    "    preprocess.transforms[-2],  # Normalize\n",
    "    preprocess.transforms[-1],  # (Optional) Additional CLIP-specific step\n",
    "])\n",
    "\n",
    "# Replace with your actual dataframe\n",
    "df = pd.read_csv(\"chameleon.csv\")\n",
    "# Ensure df has columns: 'path' and 'label'\n",
    "\n",
    "# Shuffle and split\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "split_idx = int(0.8 * len(df))\n",
    "df_80 = df.iloc[:split_idx].reset_index(drop=True)\n",
    "df_20 = df.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "# Dataset class\n",
    "class LocalImageDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['path']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Feature extractor\n",
    "def extract_clip_features(df_split, name):\n",
    "    dataset = LocalImageDataset(df_split, custom_preprocess)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0)  # âœ… Set to 0 for macOS\n",
    "\n",
    "    features, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, lbls) in enumerate(dataloader):\n",
    "            print(f\"Batch {i+1}/{len(dataloader)}\")\n",
    "            imgs = imgs.to(device)\n",
    "            clip_feats = model.encode_image(imgs).cpu().numpy()\n",
    "            features.append(clip_feats)\n",
    "            labels.append(lbls.numpy())\n",
    "\n",
    "    X = np.vstack(features)\n",
    "    y = np.concatenate(labels)\n",
    "\n",
    "    np.save(f\"chameleon_clip_features_{name}.npy\", X)\n",
    "    np.save(f\"chameleon_clip_labels_{name}.npy\", y)\n",
    "    print(f\"âœ”ï¸ Saved clip_features_{name}.npy and clip_labels_{name}.npy\")\n",
    "\n",
    "# Extract features\n",
    "extract_clip_features(df_80, \"train\")\n",
    "extract_clip_features(df_20, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T15:51:34.921537Z",
     "iopub.status.busy": "2025-04-23T15:51:34.921040Z",
     "iopub.status.idle": "2025-04-23T15:53:41.542563Z",
     "shell.execute_reply": "2025-04-23T15:53:41.541799Z",
     "shell.execute_reply.started": "2025-04-23T15:51:34.921501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Epoch 1/200, Loss: 5.3319, Accuracy: 70.28%\n",
      "[Finetune] Epoch 2/200, Loss: 4.8904, Accuracy: 71.83%\n",
      "[Finetune] Epoch 3/200, Loss: 4.3315, Accuracy: 72.99%\n",
      "[Finetune] Epoch 4/200, Loss: 4.1391, Accuracy: 74.55%\n",
      "[Finetune] Epoch 5/200, Loss: 3.5539, Accuracy: 75.76%\n",
      "[Finetune] Epoch 6/200, Loss: 3.3672, Accuracy: 76.39%\n",
      "[Finetune] Epoch 7/200, Loss: 2.9168, Accuracy: 77.51%\n",
      "[Finetune] Epoch 8/200, Loss: 2.7403, Accuracy: 78.73%\n",
      "[Finetune] Epoch 9/200, Loss: 2.4909, Accuracy: 79.19%\n",
      "[Finetune] Epoch 10/200, Loss: 2.4405, Accuracy: 80.01%\n",
      "[Finetune] Epoch 11/200, Loss: 2.2548, Accuracy: 80.68%\n",
      "[Finetune] Epoch 12/200, Loss: 2.0536, Accuracy: 81.28%\n",
      "[Finetune] Epoch 13/200, Loss: 1.8649, Accuracy: 81.72%\n",
      "[Finetune] Epoch 14/200, Loss: 1.8843, Accuracy: 82.21%\n",
      "[Finetune] Epoch 15/200, Loss: 1.6699, Accuracy: 83.04%\n",
      "[Finetune] Epoch 16/200, Loss: 1.5740, Accuracy: 83.83%\n",
      "[Finetune] Epoch 17/200, Loss: 1.5165, Accuracy: 83.87%\n",
      "[Finetune] Epoch 18/200, Loss: 1.4103, Accuracy: 84.13%\n",
      "[Finetune] Epoch 19/200, Loss: 1.2885, Accuracy: 84.76%\n",
      "[Finetune] Epoch 20/200, Loss: 1.2499, Accuracy: 84.94%\n",
      "[Finetune] Epoch 21/200, Loss: 1.1797, Accuracy: 85.46%\n",
      "[Finetune] Epoch 22/200, Loss: 1.1780, Accuracy: 85.83%\n",
      "[Finetune] Epoch 23/200, Loss: 1.0458, Accuracy: 86.33%\n",
      "[Finetune] Epoch 24/200, Loss: 1.0727, Accuracy: 86.17%\n",
      "[Finetune] Epoch 25/200, Loss: 0.9523, Accuracy: 86.53%\n",
      "[Finetune] Epoch 26/200, Loss: 0.9460, Accuracy: 86.92%\n",
      "[Finetune] Epoch 27/200, Loss: 0.9320, Accuracy: 87.34%\n",
      "[Finetune] Epoch 28/200, Loss: 0.9012, Accuracy: 87.42%\n",
      "[Finetune] Epoch 29/200, Loss: 0.8586, Accuracy: 87.45%\n",
      "[Finetune] Epoch 30/200, Loss: 0.8232, Accuracy: 87.99%\n",
      "[Finetune] Epoch 31/200, Loss: 0.7553, Accuracy: 88.31%\n",
      "[Finetune] Epoch 32/200, Loss: 0.7472, Accuracy: 88.53%\n",
      "[Finetune] Epoch 33/200, Loss: 0.6575, Accuracy: 88.47%\n",
      "[Finetune] Epoch 34/200, Loss: 0.6644, Accuracy: 88.57%\n",
      "[Finetune] Epoch 35/200, Loss: 0.6610, Accuracy: 88.94%\n",
      "[Finetune] Epoch 36/200, Loss: 0.6209, Accuracy: 89.11%\n",
      "[Finetune] Epoch 37/200, Loss: 0.6449, Accuracy: 89.64%\n",
      "[Finetune] Epoch 38/200, Loss: 0.5823, Accuracy: 89.31%\n",
      "[Finetune] Epoch 39/200, Loss: 0.5642, Accuracy: 89.40%\n",
      "[Finetune] Epoch 40/200, Loss: 0.5651, Accuracy: 89.91%\n",
      "[Finetune] Epoch 41/200, Loss: 0.5054, Accuracy: 89.92%\n",
      "[Finetune] Epoch 42/200, Loss: 0.5289, Accuracy: 89.74%\n",
      "[Finetune] Epoch 43/200, Loss: 0.4935, Accuracy: 90.27%\n",
      "[Finetune] Epoch 44/200, Loss: 0.5150, Accuracy: 90.23%\n",
      "[Finetune] Epoch 45/200, Loss: 0.4813, Accuracy: 90.45%\n",
      "[Finetune] Epoch 46/200, Loss: 0.4687, Accuracy: 90.61%\n",
      "[Finetune] Epoch 47/200, Loss: 0.4567, Accuracy: 90.92%\n",
      "[Finetune] Epoch 48/200, Loss: 0.4381, Accuracy: 90.69%\n",
      "[Finetune] Epoch 49/200, Loss: 0.4392, Accuracy: 90.82%\n",
      "[Finetune] Epoch 50/200, Loss: 0.4357, Accuracy: 90.85%\n",
      "[Finetune] Epoch 51/200, Loss: 0.4192, Accuracy: 91.31%\n",
      "[Finetune] Epoch 52/200, Loss: 0.3878, Accuracy: 91.14%\n",
      "[Finetune] Epoch 53/200, Loss: 0.4201, Accuracy: 91.29%\n",
      "[Finetune] Epoch 54/200, Loss: 0.4089, Accuracy: 91.08%\n",
      "[Finetune] Epoch 55/200, Loss: 0.3673, Accuracy: 91.56%\n",
      "[Finetune] Epoch 56/200, Loss: 0.4041, Accuracy: 91.41%\n",
      "[Finetune] Epoch 57/200, Loss: 0.3890, Accuracy: 91.82%\n",
      "[Finetune] Epoch 58/200, Loss: 0.3539, Accuracy: 91.99%\n",
      "[Finetune] Epoch 59/200, Loss: 0.3431, Accuracy: 91.80%\n",
      "[Finetune] Epoch 60/200, Loss: 0.3657, Accuracy: 91.84%\n",
      "[Finetune] Epoch 61/200, Loss: 0.3221, Accuracy: 91.87%\n",
      "[Finetune] Epoch 62/200, Loss: 0.3404, Accuracy: 92.16%\n",
      "[Finetune] Epoch 63/200, Loss: 0.3375, Accuracy: 91.79%\n",
      "[Finetune] Epoch 64/200, Loss: 0.3162, Accuracy: 92.13%\n",
      "[Finetune] Epoch 65/200, Loss: 0.3146, Accuracy: 92.27%\n",
      "[Finetune] Epoch 66/200, Loss: 0.3182, Accuracy: 92.49%\n",
      "[Finetune] Epoch 67/200, Loss: 0.3218, Accuracy: 92.28%\n",
      "[Finetune] Epoch 68/200, Loss: 0.3164, Accuracy: 92.34%\n",
      "[Finetune] Epoch 69/200, Loss: 0.3063, Accuracy: 92.42%\n",
      "[Finetune] Epoch 70/200, Loss: 0.2752, Accuracy: 92.56%\n",
      "[Finetune] Epoch 71/200, Loss: 0.2658, Accuracy: 92.56%\n",
      "[Finetune] Epoch 72/200, Loss: 0.2931, Accuracy: 92.64%\n",
      "[Finetune] Epoch 73/200, Loss: 0.2886, Accuracy: 92.65%\n",
      "[Finetune] Epoch 74/200, Loss: 0.2807, Accuracy: 92.89%\n",
      "[Finetune] Epoch 75/200, Loss: 0.2612, Accuracy: 92.80%\n",
      "[Finetune] Epoch 76/200, Loss: 0.2725, Accuracy: 92.72%\n",
      "[Finetune] Epoch 77/200, Loss: 0.2735, Accuracy: 92.99%\n",
      "[Finetune] Epoch 78/200, Loss: 0.2533, Accuracy: 93.02%\n",
      "[Finetune] Epoch 79/200, Loss: 0.2568, Accuracy: 93.10%\n",
      "[Finetune] Epoch 80/200, Loss: 0.2371, Accuracy: 93.23%\n",
      "[Finetune] Epoch 81/200, Loss: 0.2529, Accuracy: 93.39%\n",
      "[Finetune] Epoch 82/200, Loss: 0.2514, Accuracy: 93.02%\n",
      "[Finetune] Epoch 83/200, Loss: 0.2607, Accuracy: 93.12%\n",
      "[Finetune] Epoch 84/200, Loss: 0.2616, Accuracy: 93.19%\n",
      "[Finetune] Epoch 85/200, Loss: 0.2478, Accuracy: 93.15%\n",
      "[Finetune] Epoch 86/200, Loss: 0.2353, Accuracy: 93.39%\n",
      "[Finetune] Epoch 87/200, Loss: 0.2379, Accuracy: 93.24%\n",
      "[Finetune] Epoch 88/200, Loss: 0.2318, Accuracy: 93.37%\n",
      "[Finetune] Epoch 89/200, Loss: 0.2403, Accuracy: 93.29%\n",
      "[Finetune] Epoch 90/200, Loss: 0.2296, Accuracy: 93.58%\n",
      "[Finetune] Epoch 91/200, Loss: 0.2279, Accuracy: 93.64%\n",
      "[Finetune] Epoch 92/200, Loss: 0.2232, Accuracy: 93.64%\n",
      "[Finetune] Epoch 93/200, Loss: 0.2153, Accuracy: 93.46%\n",
      "[Finetune] Epoch 94/200, Loss: 0.2219, Accuracy: 93.65%\n",
      "[Finetune] Epoch 95/200, Loss: 0.2264, Accuracy: 93.78%\n",
      "[Finetune] Epoch 96/200, Loss: 0.2104, Accuracy: 93.48%\n",
      "[Finetune] Epoch 97/200, Loss: 0.2149, Accuracy: 93.57%\n",
      "[Finetune] Epoch 98/200, Loss: 0.2102, Accuracy: 93.55%\n",
      "[Finetune] Epoch 99/200, Loss: 0.2120, Accuracy: 93.98%\n",
      "[Finetune] Epoch 100/200, Loss: 0.2075, Accuracy: 93.95%\n",
      "[Finetune] Epoch 101/200, Loss: 0.2058, Accuracy: 93.85%\n",
      "[Finetune] Epoch 102/200, Loss: 0.2148, Accuracy: 93.52%\n",
      "[Finetune] Epoch 103/200, Loss: 0.2143, Accuracy: 93.68%\n",
      "[Finetune] Epoch 104/200, Loss: 0.1958, Accuracy: 93.95%\n",
      "[Finetune] Epoch 105/200, Loss: 0.2050, Accuracy: 94.04%\n",
      "[Finetune] Epoch 106/200, Loss: 0.1965, Accuracy: 94.01%\n",
      "[Finetune] Epoch 107/200, Loss: 0.2012, Accuracy: 94.39%\n",
      "[Finetune] Epoch 108/200, Loss: 0.1926, Accuracy: 94.08%\n",
      "[Finetune] Epoch 109/200, Loss: 0.2017, Accuracy: 94.00%\n",
      "[Finetune] Epoch 110/200, Loss: 0.1889, Accuracy: 94.07%\n",
      "[Finetune] Epoch 111/200, Loss: 0.1899, Accuracy: 93.98%\n",
      "[Finetune] Epoch 112/200, Loss: 0.1947, Accuracy: 94.03%\n",
      "[Finetune] Epoch 113/200, Loss: 0.1811, Accuracy: 94.21%\n",
      "[Finetune] Epoch 114/200, Loss: 0.1854, Accuracy: 94.22%\n",
      "[Finetune] Epoch 115/200, Loss: 0.1827, Accuracy: 94.31%\n",
      "[Finetune] Epoch 116/200, Loss: 0.1830, Accuracy: 94.30%\n",
      "[Finetune] Epoch 117/200, Loss: 0.1859, Accuracy: 94.41%\n",
      "[Finetune] Epoch 118/200, Loss: 0.1773, Accuracy: 94.05%\n",
      "[Finetune] Epoch 119/200, Loss: 0.1697, Accuracy: 94.31%\n",
      "[Finetune] Epoch 120/200, Loss: 0.1918, Accuracy: 94.14%\n",
      "[Finetune] Epoch 121/200, Loss: 0.1645, Accuracy: 94.47%\n",
      "[Finetune] Epoch 122/200, Loss: 0.1763, Accuracy: 94.45%\n",
      "[Finetune] Epoch 123/200, Loss: 0.1681, Accuracy: 94.33%\n",
      "[Finetune] Epoch 124/200, Loss: 0.1712, Accuracy: 94.47%\n",
      "[Finetune] Epoch 125/200, Loss: 0.1748, Accuracy: 94.44%\n",
      "[Finetune] Epoch 126/200, Loss: 0.1773, Accuracy: 94.50%\n",
      "[Finetune] Epoch 127/200, Loss: 0.1666, Accuracy: 94.47%\n",
      "[Finetune] Epoch 128/200, Loss: 0.1801, Accuracy: 94.57%\n",
      "[Finetune] Epoch 129/200, Loss: 0.1697, Accuracy: 94.42%\n",
      "[Finetune] Epoch 130/200, Loss: 0.1578, Accuracy: 94.65%\n",
      "[Finetune] Epoch 131/200, Loss: 0.1696, Accuracy: 94.45%\n",
      "[Finetune] Epoch 132/200, Loss: 0.1866, Accuracy: 94.60%\n",
      "[Finetune] Epoch 133/200, Loss: 0.1607, Accuracy: 94.65%\n",
      "[Finetune] Epoch 134/200, Loss: 0.1637, Accuracy: 94.58%\n",
      "[Finetune] Epoch 135/200, Loss: 0.1605, Accuracy: 94.55%\n",
      "[Finetune] Epoch 136/200, Loss: 0.1571, Accuracy: 94.64%\n",
      "[Finetune] Epoch 137/200, Loss: 0.1615, Accuracy: 94.71%\n",
      "[Finetune] Epoch 138/200, Loss: 0.1693, Accuracy: 94.71%\n",
      "[Finetune] Epoch 139/200, Loss: 0.1522, Accuracy: 94.84%\n",
      "[Finetune] Epoch 140/200, Loss: 0.1582, Accuracy: 94.70%\n",
      "[Finetune] Epoch 141/200, Loss: 0.1695, Accuracy: 95.05%\n",
      "[Finetune] Epoch 142/200, Loss: 0.1634, Accuracy: 94.85%\n",
      "[Finetune] Epoch 143/200, Loss: 0.1521, Accuracy: 94.62%\n",
      "[Finetune] Epoch 144/200, Loss: 0.1480, Accuracy: 94.76%\n",
      "[Finetune] Epoch 145/200, Loss: 0.1521, Accuracy: 94.91%\n",
      "[Finetune] Epoch 146/200, Loss: 0.1491, Accuracy: 94.97%\n",
      "[Finetune] Epoch 147/200, Loss: 0.1442, Accuracy: 94.80%\n",
      "[Finetune] Epoch 148/200, Loss: 0.1551, Accuracy: 95.08%\n",
      "[Finetune] Epoch 149/200, Loss: 0.1418, Accuracy: 95.15%\n",
      "[Finetune] Epoch 150/200, Loss: 0.1498, Accuracy: 95.00%\n",
      "[Finetune] Epoch 151/200, Loss: 0.1520, Accuracy: 94.98%\n",
      "[Finetune] Epoch 152/200, Loss: 0.1537, Accuracy: 94.91%\n",
      "[Finetune] Epoch 153/200, Loss: 0.1439, Accuracy: 95.08%\n",
      "[Finetune] Epoch 154/200, Loss: 0.1442, Accuracy: 95.02%\n",
      "[Finetune] Epoch 155/200, Loss: 0.1380, Accuracy: 95.08%\n",
      "[Finetune] Epoch 156/200, Loss: 0.1374, Accuracy: 95.21%\n",
      "[Finetune] Epoch 157/200, Loss: 0.1429, Accuracy: 95.07%\n",
      "[Finetune] Epoch 158/200, Loss: 0.1384, Accuracy: 95.26%\n",
      "[Finetune] Epoch 159/200, Loss: 0.1384, Accuracy: 95.16%\n",
      "[Finetune] Epoch 160/200, Loss: 0.1430, Accuracy: 95.18%\n",
      "[Finetune] Epoch 161/200, Loss: 0.1342, Accuracy: 95.36%\n",
      "[Finetune] Epoch 162/200, Loss: 0.1430, Accuracy: 95.18%\n",
      "[Finetune] Epoch 163/200, Loss: 0.1559, Accuracy: 95.14%\n",
      "[Finetune] Epoch 164/200, Loss: 0.1392, Accuracy: 95.32%\n",
      "[Finetune] Epoch 165/200, Loss: 0.1422, Accuracy: 95.28%\n",
      "[Finetune] Epoch 166/200, Loss: 0.1441, Accuracy: 95.29%\n",
      "[Finetune] Epoch 167/200, Loss: 0.1417, Accuracy: 95.22%\n",
      "[Finetune] Epoch 168/200, Loss: 0.1371, Accuracy: 95.21%\n",
      "[Finetune] Epoch 169/200, Loss: 0.1481, Accuracy: 95.31%\n",
      "[Finetune] Epoch 170/200, Loss: 0.1340, Accuracy: 95.46%\n",
      "[Finetune] Epoch 171/200, Loss: 0.1332, Accuracy: 95.44%\n",
      "[Finetune] Epoch 172/200, Loss: 0.1403, Accuracy: 95.42%\n",
      "[Finetune] Epoch 173/200, Loss: 0.1329, Accuracy: 95.49%\n",
      "[Finetune] Epoch 174/200, Loss: 0.1365, Accuracy: 95.58%\n",
      "[Finetune] Epoch 175/200, Loss: 0.1260, Accuracy: 95.70%\n",
      "[Finetune] Epoch 176/200, Loss: 0.1337, Accuracy: 95.28%\n",
      "[Finetune] Epoch 177/200, Loss: 0.1254, Accuracy: 95.73%\n",
      "[Finetune] Epoch 178/200, Loss: 0.1353, Accuracy: 95.51%\n",
      "[Finetune] Epoch 179/200, Loss: 0.1299, Accuracy: 95.62%\n",
      "[Finetune] Epoch 180/200, Loss: 0.1390, Accuracy: 95.34%\n",
      "[Finetune] Epoch 181/200, Loss: 0.1253, Accuracy: 95.43%\n",
      "[Finetune] Epoch 182/200, Loss: 0.1305, Accuracy: 95.56%\n",
      "[Finetune] Epoch 183/200, Loss: 0.1272, Accuracy: 95.65%\n",
      "[Finetune] Epoch 184/200, Loss: 0.1335, Accuracy: 95.47%\n",
      "[Finetune] Epoch 185/200, Loss: 0.1231, Accuracy: 95.49%\n",
      "[Finetune] Epoch 186/200, Loss: 0.1284, Accuracy: 95.58%\n",
      "[Finetune] Epoch 187/200, Loss: 0.1278, Accuracy: 95.74%\n",
      "[Finetune] Epoch 188/200, Loss: 0.1385, Accuracy: 95.63%\n",
      "[Finetune] Epoch 189/200, Loss: 0.1273, Accuracy: 95.50%\n",
      "[Finetune] Epoch 190/200, Loss: 0.1216, Accuracy: 95.61%\n",
      "[Finetune] Epoch 191/200, Loss: 0.1223, Accuracy: 95.77%\n",
      "[Finetune] Epoch 192/200, Loss: 0.1169, Accuracy: 95.62%\n",
      "[Finetune] Epoch 193/200, Loss: 0.1181, Accuracy: 95.85%\n",
      "[Finetune] Epoch 194/200, Loss: 0.1319, Accuracy: 96.03%\n",
      "[Finetune] Epoch 195/200, Loss: 0.1163, Accuracy: 95.67%\n",
      "[Finetune] Epoch 196/200, Loss: 0.1226, Accuracy: 95.84%\n",
      "[Finetune] Epoch 197/200, Loss: 0.1215, Accuracy: 95.75%\n",
      "[Finetune] Epoch 198/200, Loss: 0.1131, Accuracy: 96.00%\n",
      "[Finetune] Epoch 199/200, Loss: 0.1157, Accuracy: 95.96%\n",
      "[Finetune] Epoch 200/200, Loss: 0.1187, Accuracy: 95.57%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ------------------------------\n",
    "# Load NEW Training Data\n",
    "# ------------------------------\n",
    "features_new = np.load(\"/kaggle/input/chameleon-few-shot/chameleon_clip_features_train.npy\")\n",
    "labels_new = np.load(\"/kaggle/input/chameleon-few-shot/chameleon_clip_labels_train.npy\")\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "features_new = scaler.fit_transform(features_new)\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(len(features_new))\n",
    "features_new = features_new[indices]\n",
    "labels_new = labels_new[indices]\n",
    "\n",
    "# Convert to tensors\n",
    "features_new = torch.tensor(features_new, dtype=torch.float32)\n",
    "labels_new = torch.tensor(labels_new, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_new = DataLoader(TensorDataset(features_new, labels_new), batch_size=64, shuffle=True)\n",
    "\n",
    "# Make sure model, criterion, optimizer are already defined and not reinitialized\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)  # Lower learning rate\n",
    "\n",
    "# Make sure model, criterion, optimizer are already defined and not reinitialized\n",
    "num_epochs_finetune = 100  # Or however many you want\n",
    "\n",
    "for epoch in range(num_epochs_finetune):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for X_batch, y_batch in train_loader_new:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = (outputs > 0.5).float()\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"[Finetune] Epoch {epoch+1}/{num_epochs_finetune}, Loss: {total_loss/len(train_loader_new):.4f}, Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on remaining Chameleon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T16:00:25.505545Z",
     "iopub.status.busy": "2025-04-23T16:00:25.505261Z",
     "iopub.status.idle": "2025-04-23T16:00:25.662039Z",
     "shell.execute_reply": "2025-04-23T16:00:25.661101Z",
     "shell.execute_reply.started": "2025-04-23T16:00:25.505524Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on cham 20% :\n",
      "Accuracy:  0.9430\n",
      "Precision: 0.9897\n",
      "Recall:    0.8764\n",
      "F1 Score:  0.9296\n",
      "CPU times: user 187 ms, sys: 46.5 ms, total: 233 ms\n",
      "Wall time: 149 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load 20% features and labels\n",
    "features_20 = np.load(\"/kaggle/input/chameleon-few-shot/chameleon_clip_features_test.npy\")\n",
    "labels_20 = np.load(\"/kaggle/input/chameleon-few-shot/chameleon_clip_labels_test.npy\")\n",
    "\n",
    "# Apply the same scaler used during training\n",
    "features_20 = scaler.transform(features_20)\n",
    "\n",
    "# Convert to torch tensors\n",
    "features_20 = torch.tensor(features_20, dtype=torch.float32).to(device)\n",
    "labels_20 = torch.tensor(labels_20, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(features_20)\n",
    "    preds = (outputs > 0.5).float()\n",
    "\n",
    "# Move to CPU and flatten\n",
    "y_true = labels_20.cpu().numpy().flatten()\n",
    "y_pred = preds.cpu().numpy().flatten()\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Evaluation on cham 20% :\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test chameleon set on other benchmark models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using model: tf_efficientnetv2_s.in1k\n",
      "âœ… Using model: vit_base_patch16_224.augreg_in21k\n",
      "\n",
      "ðŸ”¹ Fine-tuning ResNet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Loss: 0.2649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Loss: 0.1384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Loss: 0.0924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 â†’ {'Acc': 0.9476837981101637, 'F1': 0.9377911756645656, 'Prec': 0.9630393996247655, 'Rec': 0.9138330069432081}\n",
      "\n",
      "ðŸ”¹ Fine-tuning EffNetV2-B3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Loss: 0.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Loss: 0.1247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Loss: 0.0766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EffNetV2-B3 â†’ {'Acc': 0.9532150265038027, 'F1': 0.9443683200876952, 'Prec': 0.9697936210131332, 'Rec': 0.9202421221292505}\n",
      "\n",
      "ðŸ”¹ Fine-tuning ViT-B/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Loss: 0.3647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Loss: 0.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Loss: 0.2215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-B/16 â†’ {'Acc': 0.8705538910655297, 'F1': 0.8316178674927551, 'Prec': 0.9478359908883827, 'Rec': 0.740786896920064}\n",
      "\n",
      "ðŸ”¹ Fine-tuning CLIP-ViT-L/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPooling' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 176\u001b[0m\n\u001b[1;32m    173\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 176\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    179\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader)\n",
      "Cell \u001b[0;32mIn[6], line 127\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m    125\u001b[0m imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    126\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 127\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    129\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/pooling.py:1455\u001b[0m, in \u001b[0;36mAdaptiveAvgPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:1381\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[0;34m(input, output_size)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size)\n\u001b[0;32m-> 1381\u001b[0m _output_size \u001b[38;5;241m=\u001b[39m _list_with_default(output_size, \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(\u001b[38;5;28minput\u001b[39m, _output_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPooling' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "chameleon_benchmark_train_eval.py\n",
    "---------------------------------\n",
    "Fine-tune and evaluate common deepfake detection baselines (ResNet50, EffNetV2-B3,\n",
    "ViT-B/16, CLIP-ViT-L/14) on the Chameleon dataset.\n",
    "\n",
    "Expected CSV columns: filepath,label\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import CLIPModel\n",
    "import timm\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "CSV_PATH = \"chameleon.csv\"   # has filepath,label\n",
    "BATCH_SIZE = 8               # reduce to prevent OOM\n",
    "EPOCHS = 3                   # quick fine-tune; adjust as needed\n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SPLIT_RATIO = 0.5            # 50% train / 50% test\n",
    "# ====================================\n",
    "\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx][\"filepath\"]\n",
    "        label = int(self.df.iloc[idx][\"label\"])\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# ---------- Transforms ----------\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def get_benchmark_models():\n",
    "    models_dict = {}\n",
    "    available = timm.list_models(pretrained=True)\n",
    "\n",
    "    def safe_timm_model(preferred_names, fallback=\"efficientnet_b3\"):\n",
    "        \"\"\"Try multiple model names safely.\"\"\"\n",
    "        for name in preferred_names:\n",
    "            if name in available:\n",
    "                print(f\"âœ… Using model: {name}\")\n",
    "                return timm.create_model(name, pretrained=True)\n",
    "        print(f\"âš ï¸ None of {preferred_names} found. Falling back to {fallback}\")\n",
    "        return timm.create_model(fallback, pretrained=True)\n",
    "\n",
    "    # ResNet50\n",
    "    m1 = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "    m1.fc = nn.Linear(m1.fc.in_features, 1)\n",
    "    models_dict[\"ResNet50\"] = m1\n",
    "\n",
    "    # EfficientNetV2-B3 (timm)\n",
    "    m2 = safe_timm_model([\n",
    "        \"efficientnetv2_b3\",\n",
    "        \"tf_efficientnetv2_s.in1k\",\n",
    "        \"efficientnet_b3\"\n",
    "    ])\n",
    "    in_features = m2.get_classifier().in_features\n",
    "    m2.classifier = nn.Linear(in_features, 1)\n",
    "    models_dict[\"EffNetV2-B3\"] = m2\n",
    "\n",
    "    # ViT-B/16\n",
    "    m3 = safe_timm_model([\n",
    "        \"vit_base_patch16_224\",\n",
    "        \"vit_base_patch16_224.augreg_in21k\",\n",
    "        \"vit_base_patch16_224.in1k\"\n",
    "    ])\n",
    "    in_features = m3.head.in_features\n",
    "    m3.head = nn.Linear(in_features, 1)\n",
    "    models_dict[\"ViT-B/16\"] = m3\n",
    "\n",
    "    # CLIP ViT-L/14\n",
    "    clip = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    visual = clip.vision_model\n",
    "    head = nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(visual.config.hidden_size, 1)\n",
    "    )\n",
    "    model = nn.Sequential(visual, head)\n",
    "    models_dict[\"CLIP-ViT-L/14\"] = model\n",
    "\n",
    "    return models_dict\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs, labels = imgs.to(DEVICE), labels.float().unsqueeze(1).to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"Testing\", leave=False):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy().ravel()\n",
    "            preds.extend((probs > 0.5).astype(int))\n",
    "            trues.extend(labels.numpy())\n",
    "\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds)\n",
    "    prec = precision_score(trues, preds)\n",
    "    rec = recall_score(trues, preds)\n",
    "    return {\"Acc\": acc, \"F1\": f1, \"Prec\": prec, \"Rec\": rec}\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Dataset split\n",
    "    full_dataset = ImageDataset(CSV_PATH, transform=train_tfm)\n",
    "    n_train = int(SPLIT_RATIO * len(full_dataset))\n",
    "    n_test = len(full_dataset) - n_train\n",
    "    train_set, test_set = random_split(full_dataset, [n_train, n_test])\n",
    "\n",
    "    test_set.dataset.transform = test_tfm\n",
    "\n",
    "    # num_workers=0 avoids macOS pickle error\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    all_results = []\n",
    "    for name, model in get_benchmark_models().items():\n",
    "        print(f\"\\nðŸ”¹ Fine-tuning {name}\")\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {loss:.4f}\")\n",
    "\n",
    "        metrics = evaluate_model(model, test_loader)\n",
    "        print(f\"{name} â†’ {metrics}\")\n",
    "        all_results.append({\"Model\": name, **metrics})\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(\"benchmark_results_chameleon_finetuned.csv\", index=False)\n",
    "    print(\"\\nâœ… Saved fine-tuned benchmark results â†’ benchmark_results_chameleon_finetuned.csv\")\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10884264,
     "sourceId": 91198,
     "sourceType": "competition"
    },
    {
     "datasetId": 6412205,
     "sourceId": 10550636,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6724337,
     "sourceId": 10829354,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6724726,
     "sourceId": 10829922,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6834512,
     "sourceId": 10981960,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6836078,
     "sourceId": 10983980,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7224568,
     "sourceId": 11519427,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7230645,
     "sourceId": 11528447,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7231939,
     "sourceId": 11530141,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 250940,
     "modelInstanceId": 229192,
     "sourceId": 267811,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
