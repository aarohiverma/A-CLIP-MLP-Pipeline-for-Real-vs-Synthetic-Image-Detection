{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/aarohiverma/Documents/ai_vs_real/train.csv')\n",
    "df = df.drop(columns ='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetV2B3\n",
    "from tensorflow.keras.layers import (Dense, GlobalAveragePooling2D, Input, \n",
    "                                     Dropout, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os  \n",
    "\n",
    "# ----- Constants -----\n",
    "IMG_SIZE = 256  \n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.000005\n",
    "EPOCHS = 30  \n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "IMAGE_BASE_PATH = \"\"  \n",
    "\n",
    "# ----- Load DataFrame -----\n",
    "# df = pd.read_csv(\"your_csv_file.csv\")  # Uncomment if needed\n",
    "df[\"file_name\"] = df[\"file_name\"].apply(lambda x: os.path.join(IMAGE_BASE_PATH, x))\n",
    "file_paths = df[\"file_name\"].values  \n",
    "labels = df[\"label\"].values.astype(np.float32)\n",
    "\n",
    "# ----- Train-Test Split -----\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    file_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# ----- Data Augmentation -----\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomBrightness(0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "def load_image(file_path, label, augment=False):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE)) / 255.0\n",
    "    if augment:\n",
    "        img = data_augmentation(img)\n",
    "    return img, label\n",
    "\n",
    "# ----- TF Datasets -----\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_dataset = train_dataset.map(lambda x, y: load_image(x, y, True), num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(5000).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "val_dataset = val_dataset.map(lambda x, y: load_image(x, y, False), num_parallel_calls=AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# ----- Raw CNN Model (Without Self-Attention) -----\n",
    "image_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"image_input\")\n",
    "base_model = EfficientNetV2B3(weights=\"imagenet\", include_top=False)(image_input)\n",
    "image_features = GlobalAveragePooling2D()(base_model)\n",
    "\n",
    "x = Dense(128, activation=\"relu\", kernel_regularizer=l2(0.0005))(image_features)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=image_input, outputs=output)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", Recall(name=\"recall\")]\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# ----- Train -----\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=[early_stop])\n",
    "\n",
    "# ----- Evaluate -----\n",
    "# Get predictions from validation set\n",
    "val_images = []\n",
    "val_labels_list = []\n",
    "for img_batch, label_batch in val_dataset:\n",
    "    val_images.append(img_batch)\n",
    "    val_labels_list.append(label_batch)\n",
    "val_images = tf.concat(val_images, axis=0)\n",
    "val_labels = tf.concat(val_labels_list, axis=0)\n",
    "\n",
    "# Predict\n",
    "pred_probs = model.predict(val_images, batch_size=BATCH_SIZE)\n",
    "pred_labels = (pred_probs > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, pred_labels))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, pred_labels, digits=4))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
